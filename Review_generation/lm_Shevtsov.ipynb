{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем какие-нибудь сырые данные и сделаем модель для генерации текста. \n",
    "\n",
    "Спойлер: я построил модель где в качестве токенов использовал слова, а не символы, хотя получившаяся модель на символах у меня генериала отдельные слова (именно слова а не белеберду) ее обучение на моем компьютере занимало слишком много времени, полтора часа на одну эпоху, поэтому использовать в качестве токенизации слова принесло мне больший импрув, такой как ускорение обучения за счет уменьшения обрабатывемой последовательности (timestamp) в GRU и более осмысленный текст на выходе. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве корпуса текста я собрал готовые сочинения по произведению Пушкина \"Евгений Онегин\" с сайта litra.ru\n",
    "\n",
    "Сочинения по произведению \"Евгений Онегин\"- http://www.litra.ru/composition/work/woid/00028601184773070301/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_url = 'http://www.litra.ru/composition/work/woid/00028601184773070301/'\n",
    "\n",
    "sess = requests.Session()\n",
    "g = sess.get(general_url)\n",
    "b=BeautifulSoup(g.text, \"html.parser\")\n",
    "element = b.find('div', {'id': 'child_id2'})\n",
    "urles = re.findall('<a href=[^>]+', str(element))\n",
    "\n",
    "urles = ['http://www.litra.ru' + x.replace('<a href=\"', '').replace('\"', '') for x in urles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.litra.ru/composition/get/coid/00160151213701291695/woid/00028601184773070301/',\n",
       " 'http://www.litra.ru/composition/get/coid/00853061230017597355/woid/00028601184773070301/',\n",
       " 'http://www.litra.ru/composition/get/coid/00061101184864166554/woid/00028601184773070301/',\n",
       " 'http://www.litra.ru/composition/get/coid/00043801184864193473/woid/00028601184773070301/',\n",
       " 'http://www.litra.ru/composition/get/coid/00067801184864021931/woid/00028601184773070301/']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выгрузим сочинения по полученным ссылкам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f52311bf83442e94eff60bbde7ea5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=306), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "for i, url in tqdm_notebook(enumerate(urles), total = len(urles)):\n",
    "    g = sess.get(url)\n",
    "    b=BeautifulSoup(g.text, \"html.parser\")\n",
    "\n",
    "    tags_list = b.find_all('p', {'align':'justify'})\n",
    "    \n",
    "    text = re.sub('[^а-яА-Я0-9a-zA-z;?!:,. -/s]', '', tags_list[0].text)\n",
    "    text = re.sub('[-/s;:,.?!\"\"()]', ' ', text)\n",
    "    text = re.sub('[ ]+', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.strip(' ')\n",
    "    \n",
    "    texts.append(text)\n",
    "    with open('reviews/review_' + ('%03d' % i) + '.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "        \n",
    "    time.sleep(1) # чтобы не долбиться слишком часто на сервер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "for i in range(306):\n",
    "    with open('reviews/review_' + ('%03d' % i) + '.txt', 'r') as file:\n",
    "        texts.append(file.read())\n",
    "        \n",
    "text = ' '.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "евгений онегин , пожалуй , самое трудное для понимания произведение русской литературы . про него не скажешь с...\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][:110]+ '...')\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249939\n",
      "25158\n"
     ]
    }
   ],
   "source": [
    "text_words = text.split(' ')\n",
    "\n",
    "print(len(text_words))\n",
    "print(len(set(text_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{TextProcessing}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед обучением модели, нам нужно сопоставить строковое представление численному. Для этого создадим два словаря сопоставляющих слова числам и наоборот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from unique words to indxs\n",
    "word2idx = {u: v for v, u in enumerate(vocab)}\n",
    "idx2word = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([word2idx[c] for c in text_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы получили численное представление слов. Заметим что мы отобразили каждое слово в его позицию от 0 до len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['евгений', 'онегин', ','] ---- words mapped to int ---- > [ 4998 12337     5]\n",
      "4998 12337 5 ---- words mapped to int ---- > евгений онегин ,\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- words mapped to int ---- > {}'.format(text_words[:3], text_as_int[:3]))\n",
    "\n",
    "print ('{} ---- words mapped to int ---- > {}'.format(' '.join(map(str, text_as_int[:3])),\n",
    "                                                      ' '.join(map(lambda x: idx2word[x],\n",
    "                                                                   text_as_int[:3]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{The prediction task}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас есть слово, или последоваьтельность слов, какое наиболее вероятное следующее слово? Это та задача для решения которой мы тренируем модель. Вход в модель есть последовательность слов, и мы тренируем модель чтобы на выходе модели получить предсказание следующего слова на каждом шаге.\n",
    "\n",
    "Разделим текст на обучающие примеры и таргеты. Каждый обучающий пример содержит последовательность слов текста. Соответствующие таргеты включают в себя последовательность такой же длины, кроме одного сдвинутого по тексту слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'евгений онегин , пожалуй , самое трудное'\n",
      "'для понимания произведение русской литературы . про'\n",
      "'него не скажешь словами твардовского вот стихи'\n",
      "', а все понятно , все на'\n",
      "'русском языке . язык этого романа в'\n"
     ]
    }
   ],
   "source": [
    "chunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in chunks.take(5):\n",
    "    print(repr(' '.join(idx2word[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, создадим входную последовательность и таргет для всех текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = chunks.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы используем tf.data чтобы поделить текст на куски и разбить по секциям. Но перед тем как скормить эти данные в модель, нам нужно перемешать их и упаковать их в батчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 40\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся tf.keras API для создания модели и ее кастомизации так, как мы захотим. Мы определим четыре слоя для определения нашей модели:\n",
    "\n",
    " - Слой Эмбеддингов: тренируемая матрица которая отображает числа для каждого символа в вектор с размерностью embedding_dim;\n",
    " - GRU слой: тип RNN модели с layer size = units.\n",
    " - Dropout слой: регуляризация сети за счет зануления узлов сети с вероятностями rate\n",
    " - Dense слой: полносвязный слой c числом выходов равным vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, rate):\n",
    "        super(Model, self).__init__()\n",
    "        self.units = units\n",
    "        self.rate = rate\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                       return_sequences=True, \n",
    "                                       recurrent_activation='sigmoid', \n",
    "                                       recurrent_initializer='glorot_uniform', \n",
    "                                       stateful=True)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(self.rate)\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "\n",
    "        # output at every time step\n",
    "        # output shape == (batch_size, seq_length, hidden_size) \n",
    "        output = self.gru(embedding)\n",
    "        \n",
    "        dropout_output = self.dropout(output)\n",
    "\n",
    "        # The dense layer will output predictions for every time_steps(seq_length)\n",
    "        # output shape after the dense layer == (seq_length * batch_size, vocab_size)\n",
    "        prediction = self.fc(dropout_output)\n",
    "\n",
    "        # states will be used to pass at every step to the model while training\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 300\n",
    "\n",
    "# Number of RNN units\n",
    "units = 1024\n",
    "\n",
    "rate = 0.5\n",
    "\n",
    "model = Model(vocab_size, embedding_dim, units, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using adam optimizer with default arguments\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# Using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
    "def loss_function(real, preds):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(tf.TensorShape([BATCH_SIZE, seq_length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"word_generating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4678db5a53df4d62b24a1bf5296a9f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 10.1329\n",
      "Epoch 1 Batch 100 Loss 7.8617\n",
      "Epoch 1 Batch 200 Loss 7.4258\n",
      "Epoch 1 Batch 300 Loss 7.5139\n",
      "Epoch 1 Batch 400 Loss 7.0018\n",
      "Epoch 1 Batch 500 Loss 6.5758\n",
      "Epoch 1 Batch 600 Loss 7.1284\n",
      "Epoch 1 Batch 700 Loss 7.0770\n",
      "Epoch 1 Batch 800 Loss 6.9369\n",
      "\n",
      "Epoch 1 Loss 6.7458\n",
      "Time taken for 1 epoch 605.5641601085663 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1721d0e264734579b392770c6ab8ec3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 0 Loss 7.4648\n",
      "Epoch 2 Batch 100 Loss 6.4731\n",
      "Epoch 2 Batch 200 Loss 6.3311\n",
      "Epoch 2 Batch 300 Loss 6.5786\n",
      "Epoch 2 Batch 400 Loss 6.2389\n",
      "Epoch 2 Batch 500 Loss 6.1548\n",
      "Epoch 2 Batch 600 Loss 6.0610\n",
      "Epoch 2 Batch 700 Loss 5.9246\n",
      "Epoch 2 Batch 800 Loss 6.1556\n",
      "\n",
      "Epoch 2 Loss 6.1654\n",
      "Time taken for 1 epoch 12380.206770181656 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00185802c34447199a3c0b62f507884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 0 Loss 6.2223\n",
      "Epoch 3 Batch 100 Loss 5.9320\n",
      "Epoch 3 Batch 200 Loss 5.7796\n",
      "Epoch 3 Batch 300 Loss 5.5128\n",
      "Epoch 3 Batch 400 Loss 5.5671\n",
      "Epoch 3 Batch 500 Loss 6.0906\n",
      "Epoch 3 Batch 600 Loss 5.5489\n",
      "Epoch 3 Batch 700 Loss 5.0239\n",
      "Epoch 3 Batch 800 Loss 5.7456\n",
      "\n",
      "Epoch 3 Loss 5.7142\n",
      "Time taken for 1 epoch 2279.252575159073 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccc9d6b53b542db8908f0b69ecdee6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 0 Loss 5.3263\n",
      "Epoch 4 Batch 100 Loss 4.9595\n",
      "Epoch 4 Batch 200 Loss 4.7749\n",
      "Epoch 4 Batch 300 Loss 5.0907\n",
      "Epoch 4 Batch 400 Loss 4.8068\n",
      "Epoch 4 Batch 500 Loss 4.4789\n",
      "Epoch 4 Batch 600 Loss 5.0804\n",
      "Epoch 4 Batch 700 Loss 4.8896\n",
      "Epoch 4 Batch 800 Loss 4.9269\n",
      "\n",
      "Epoch 4 Loss 4.7424\n",
      "Time taken for 1 epoch 3216.463793992996 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83bae602e734126802b5e37a1eeb658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 0 Loss 4.4049\n",
      "Epoch 5 Batch 100 Loss 3.9053\n",
      "Epoch 5 Batch 200 Loss 4.1232\n",
      "Epoch 5 Batch 300 Loss 4.6333\n",
      "Epoch 5 Batch 400 Loss 4.8276\n",
      "Epoch 5 Batch 500 Loss 4.2092\n",
      "Epoch 5 Batch 600 Loss 4.1084\n",
      "Epoch 5 Batch 700 Loss 3.8712\n",
      "Epoch 5 Batch 800 Loss 4.3633\n",
      "\n",
      "Epoch 5 Loss 3.9693\n",
      "Time taken for 1 epoch 3421.8115670681 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76db309f41e343639e676cb53e44c899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 0 Loss 3.9793\n",
      "Epoch 6 Batch 100 Loss 3.8289\n",
      "Epoch 6 Batch 200 Loss 4.0254\n",
      "Epoch 6 Batch 300 Loss 3.8040\n",
      "Epoch 6 Batch 400 Loss 3.7119\n",
      "Epoch 6 Batch 500 Loss 3.8590\n",
      "Epoch 6 Batch 600 Loss 3.4516\n",
      "Epoch 6 Batch 700 Loss 3.9109\n",
      "Epoch 6 Batch 800 Loss 3.6048\n",
      "\n",
      "Epoch 6 Loss 4.0384\n",
      "Time taken for 1 epoch 3422.079575061798 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfb38d53dd14a6d9da5da39e94cc1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 0 Loss 3.2844\n",
      "Epoch 7 Batch 100 Loss 3.2995\n",
      "Epoch 7 Batch 200 Loss 3.2825\n",
      "Epoch 7 Batch 300 Loss 3.2983\n",
      "Epoch 7 Batch 400 Loss 3.3775\n",
      "Epoch 7 Batch 500 Loss 3.1249\n",
      "Epoch 7 Batch 600 Loss 3.7902\n",
      "Epoch 7 Batch 700 Loss 3.3338\n",
      "Epoch 7 Batch 800 Loss 3.2933\n",
      "\n",
      "Epoch 7 Loss 3.4825\n",
      "Time taken for 1 epoch 1697.2256598472595 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc43d4799df4cb7a0189874ae272387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 0 Loss 3.1688\n",
      "Epoch 8 Batch 100 Loss 3.4133\n",
      "Epoch 8 Batch 200 Loss 3.1844\n",
      "Epoch 8 Batch 300 Loss 3.0808\n",
      "Epoch 8 Batch 400 Loss 3.2442\n",
      "Epoch 8 Batch 500 Loss 3.1951\n",
      "Epoch 8 Batch 600 Loss 3.0616\n",
      "Epoch 8 Batch 700 Loss 3.1360\n",
      "Epoch 8 Batch 800 Loss 3.0253\n",
      "\n",
      "Epoch 8 Loss 3.1729\n",
      "Time taken for 1 epoch 606.1038897037506 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a19b528be44e828206c32d9ee45f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 0 Loss 2.7709\n",
      "Epoch 9 Batch 100 Loss 2.8727\n",
      "Epoch 9 Batch 200 Loss 2.9146\n",
      "Epoch 9 Batch 300 Loss 2.9175\n",
      "Epoch 9 Batch 400 Loss 2.8920\n",
      "Epoch 9 Batch 500 Loss 3.0812\n",
      "Epoch 9 Batch 600 Loss 2.9267\n",
      "Epoch 9 Batch 700 Loss 2.9948\n",
      "Epoch 9 Batch 800 Loss 2.7045\n",
      "\n",
      "Epoch 9 Loss 2.8344\n",
      "Time taken for 1 epoch 608.6046841144562 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3604b60cba44c33a9e7ac60441adadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 0 Loss 2.6902\n",
      "Epoch 10 Batch 100 Loss 2.5019\n",
      "Epoch 10 Batch 200 Loss 2.4867\n",
      "Epoch 10 Batch 300 Loss 3.0782\n",
      "Epoch 10 Batch 400 Loss 2.7141\n",
      "Epoch 10 Batch 500 Loss 2.6382\n",
      "Epoch 10 Batch 600 Loss 2.5876\n",
      "Epoch 10 Batch 700 Loss 2.5639\n",
      "Epoch 10 Batch 800 Loss 2.8332\n",
      "\n",
      "Epoch 10 Loss 2.7728\n",
      "Time taken for 1 epoch 610.4138381481171 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initally hidden is None\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch, (inp, target)) in tqdm_notebook(enumerate(dataset)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # feeding the hidden state back into the model\n",
    "            # This is the interesting step\n",
    "            predictions = model(inp)\n",
    "            loss = loss_function(target, predictions)\n",
    "              \n",
    "            grads = tape.gradient(loss, model.variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                                            batch,\n",
    "                                                            loss))\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix)\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation step (generating text using the learned model)\n",
    "\n",
    "# Number of characters to generate\n",
    "num_generate = 200\n",
    "\n",
    "# You can change the start string to experiment\n",
    "start_string = 'роман'\n",
    "\n",
    "# Converting our start string to numbers (vectorizing) \n",
    "input_eval = [word2idx[s] for s in [start_string]*BATCH_SIZE]\n",
    "input_eval = tf.expand_dims(input_eval, 1)\n",
    "\n",
    "# Empty string to store our results\n",
    "text_generated = []\n",
    "\n",
    "# Low temperatures results in more predictable text.\n",
    "# Higher temperatures results in more surprising text.\n",
    "# Experiment to find the best setting.\n",
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "роман явился энциклопедией русской жизни ведь в этой энциклопедии заметное место заняли также описания природы которые появляются на страницах романа в стихах была поставлена проблема отсутствия достойного поля прохлада сумрачной дубравы журчанье тихого ручья на бумаге невидимые законы по своим размерам и крайне разнообразная по словам белинского а белинский татьяна хотела узнать питает ли онегин он холоден спокоен но обратимся к 8 главе он пишет покамест упивайтесь он старался сдержать свою иронию пушкин пишет ей душно здесь она мечтой стремится к жизни полевой поместный круг татьяна один день идеально вкусу наряды глубиной содержания натуры е душу и терзался оттого что никогда не были онегин это человек решивший построить свою реальную жизнь по законам европейского романа он написал но мало места уделялось чувствам пушкин работал над романом пушкин был своему приятелю наставником покровителем любовь но не настолько у тетушки княжны елены все тот же тюлевый чепец познакомившись ни потому что верна целиком москва как низкое коварство да потому что с такой полнотою светло и ясно как отразилась в онегине личность пушкина говорит белинский навсегда останется в моей памяти александр сергеевич пытается всячески поддержать пущина напоминает колкие фразы грибоедова главным образом отрицательную роль пушкин показывает как бы энциклопедию этих роковых ошибках виноваты окружающие\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop.\n",
    "\n",
    "# Here batch size == 1\n",
    "model.reset_states()\n",
    "for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "    # remove the batch dimension\n",
    "    predictions = tf.expand_dims(tf.squeeze(predictions[0], 0), 0)\n",
    "\n",
    "    # using a multinomial distribution to predict the word returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "    \n",
    "    # We pass the predicted word as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id]*BATCH_SIZE, 1)\n",
    "    \n",
    "    text_generated.append(idx2word[predicted_id])\n",
    "\n",
    "print (start_string + ' ' + ' '.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итог"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я использовал генерацию по словам, а не по символам потому что обучение на последовательностях длины 100 занимает очень много времени, около полуторачаса на эпоху. В итоге видно что текст адекватно генерится периодами, то есть слова связаны по смыслу в пределах 3-4 слов, затем начинается уже новая смысловая фраза такой же длины. Думаю чтобы это побороть нужно в процессе обучения брать более длинную последовательность. \n",
    "\n",
    "Базовую модель я взял из гайда тензорфлоу и добавил в обучение Dropout слой. Весь вышенаписанный код я реализовал в виде python модуля. Который скачивает данные с Litra.ru или берет уже из существующей папки с сочинениями, обучает и предиктит."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
